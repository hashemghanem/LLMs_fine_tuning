{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ZV9saH7WnC"
      },
      "source": [
        "# Finetune Mistral on LLAMA Factory using QLoRA\n",
        "## Base model: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "## Dataset: https://huggingface.co/datasets/MattCoddity/dockerNLcommands\n",
        "## Youtube: https://www.youtube.com/watch?v=iMD7ba1hHgw&list=PLrLEqwuz-mRIEtuUEN8sse2XyksKNN4Om&index=4&ab_channel=AIAnytime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAHll2ar8mJK"
      },
      "source": [
        "## First clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y15KsnLD7eF4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shbLBHKd7bJ9"
      },
      "outputs": [],
      "source": [
        "%cd LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAZZhPXM8jNi"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "aZEsgB0nJRuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XgevKPW9vaZ"
      },
      "source": [
        "## Install required package to run LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5QkT43pg7WTn"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9sA50Yn-l4V"
      },
      "source": [
        "### We will use a 4-bit quantization of the model and QLoRA to do the finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "azTgSEo3-rDC"
      },
      "outputs": [],
      "source": [
        "# install bitsandbytes for the quantization\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnqS81FWAFg3"
      },
      "source": [
        "## Change I to source code: add the info of your dataset\n",
        "- Go to data/dataset_info.json and add the following\n",
        "```\n",
        "  \"docker_dataset\":{\n",
        "    \"hf_hub_url\": \"MattCoddity/dockerNLcommands\",\n",
        "    \"columns\":{\n",
        "      \"prompt\":\"instruction\",\n",
        "      \"query\": \"input\",\n",
        "      \"response\":\"output\"\n",
        "    }\n",
        "  },\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16noMMSIHWw0"
      },
      "source": [
        "<details>\n",
        "  <summary style=\"background-color: #f8f9fa; padding: 10px; border: 1px solid #ccc; border-radius: 5px; font-weight: bold; cursor: pointer;\">\n",
        "    Not Necessary\n",
        "  </summary>\n",
        "  <div style=\"background-color: #f8f9fa; padding: 10px; border: 1px solid #ccc; border-radius: 5px; margin-top: 5px;\">\n",
        "    Open the source code of this markdown cell and un-comment what is below\n",
        "  </div>\n",
        "</details>\n",
        "\n",
        "<!-- ## Change II to source code: enable the userinterface on the browser via a public link (necessary since Colab has no interface)\n",
        "- Go to src/webui\n",
        "- change the definition of the variable gradio_share from\n",
        "```\n",
        " gradio_share = os.getenv(\"GRADIO_SHARE\", \"0\").lower() in [\"true\", \"1\"]\n",
        " ```\n",
        "to\n",
        " ```\n",
        " gradio_share = os.getenv(\"GRADIO_SHARE\", \"1\").lower() in [\"true\", \"1\"]\n",
        " ``` -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk0BlEgVE8Ad"
      },
      "source": [
        "## Run the user interface to setup the training parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!GRADIO_SHARE=1 llamafactory-cli webui\n",
        "# !CUDA_VISIBLE_DEVICES=0 python src/webui.py"
      ],
      "metadata": {
        "id": "fmNuqxR6j46f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPYrgARJGwhB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOys0PxqMNeb"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from subprocess import Popen\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"HF_HOME\"] = \"/root/.huggingface\"\n",
        "env[\"HF_TOKEN\"] = \"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\"  # Replace with your token\n",
        "\n",
        "Popen([\"llamafactory-cli\", \"train\", \"your_args\"], env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iESGXzgMJRO1"
      },
      "source": [
        "## Next:\n",
        "- choose model as: Mistral-7B-Instruct-v0.1\n",
        "- set Quantization bit to: 4 to enable QLoRA.\n",
        "- set prompt template to mistral\n",
        "- set learning rate to 2e-4\n",
        "- set cut of length to 512 (to reduce computating cost and time).\n",
        "- reduce max samples to 10000 (to reduce computating cost and time).\n",
        "- set epochs to 1 (to reduce computating cost and time).\n",
        "- keep using bf16 (since we are not using a powerful gpu like A100).\n",
        "- change max gradient norm = 0.3.\n",
        "- set batch size to 16\n",
        "- LoRA configuration: usually it is set automatically by the source code so we will not change it. But to play with it you can increase LoRA Rank (intuition: the smaller the model is the higher the rank should be)\n",
        "- click on preview command to see all all parameters\n",
        "- click start and monitor the losses (losses will appear after a few minutes after the model is downloaded)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge with the base model and push to you HuggingFace hub"
      ],
      "metadata": {
        "id": "IdgTXWZxfqmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aIBLgHBEVdS"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avms-7lxESOx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.1\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"/content/LLaMA-Factory/saves/Mistral-7B-v0.1/lora/train_2024-12-26-10-43-37\",            # load the saved LoRA adapters\n",
        "  template=\"mistral\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=1,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cuda\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  export_hub_model_id=\"Hghanem96/Mistral_docker\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_mistral_docker.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_mistral_docker.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZBS64YNDDsU"
      },
      "source": [
        "## Extra excercise: Try adding a new prompt template if the one of the model you finetune is not supported\n",
        "- Go to src/llamafactory/data/template\n",
        "- Templates are below in the code (ordered alphapetically by their name)\n",
        "- Add your template or modify an existing one, for example modify the following:\n",
        "```\n",
        "_register_template(\n",
        "    name=\"llama2_zh\",\n",
        "    format_user=StringFormatter(slots=[{\"bos_token\"}, \"[INST] {{content}} [/INST]\"]),\n",
        "    format_system=StringFormatter(slots=[\"<<SYS>>\\n{{content}}\\n<</SYS>>\\n\\n\"]),\n",
        "    default_system=\"You are a helpful assistant. 你是一个乐于助人的助手。\",\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}