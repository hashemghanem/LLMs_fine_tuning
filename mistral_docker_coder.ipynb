{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ZV9saH7WnC"
      },
      "source": [
        "# Finetune Mistral on LLAMA Factory using QLoRA\n",
        "### Llama Factory Supports more that 100 datasets and 50 llms, both LoRA and QLoRA and full fine-tuning.\n",
        "### Base model: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "### Dataset: https://huggingface.co/datasets/MattCoddity/dockerNLcommands\n",
        "### Youtube: https://www.youtube.com/watch?v=iMD7ba1hHgw&list=PLrLEqwuz-mRIEtuUEN8sse2XyksKNN4Om&index=4&ab_channel=AIAnytime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAHll2ar8mJK"
      },
      "source": [
        "## First clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y15KsnLD7eF4"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shbLBHKd7bJ9"
      },
      "outputs": [],
      "source": [
        "%cd LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAZZhPXM8jNi"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZEsgB0nJRuZ"
      },
      "outputs": [],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XgevKPW9vaZ"
      },
      "source": [
        "## Install required package to run LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5QkT43pg7WTn"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9sA50Yn-l4V"
      },
      "source": [
        "### We will use a 4-bit quantization of the model and QLoRA to do the finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "azTgSEo3-rDC"
      },
      "outputs": [],
      "source": [
        "# install bitsandbytes for the quantization\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Path to dataset_info.json\n",
        "file_path = \"data/dataset_info.json\"\n",
        "\n",
        "# Load the existing JSON data\n",
        "with open(file_path, \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Add the new dataset\n",
        "data[\"docker_datasett\"] = {\n",
        "    \"hf_hub_url\": \"MattCoddity/dockerNLcommands\",\n",
        "    \"columns\": {\n",
        "        \"prompt\": \"instruction\",\n",
        "        \"query\": \"input\",\n",
        "        \"response\": \"output\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the updated JSON\n",
        "with open(file_path, \"w\") as file:\n",
        "    json.dump(data, file, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk0BlEgVE8Ad"
      },
      "source": [
        "## Run the user interface to setup the training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd LLaMA-Factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Replace with your token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOys0PxqMNeb"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from subprocess import Popen\n",
        "import os\n",
        "HF_token = \"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\"\n",
        "os.environ[\"HF_TOKEN\"] = HF_token\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"HF_HOME\"] = \"/root/.huggingface\"\n",
        "env[\"HF_TOKEN\"] = HF_token \n",
        "\n",
        "Popen([\"llamafactory-cli\", \"train\", \"your_args\"], env=env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNuqxR6j46f"
      },
      "outputs": [],
      "source": [
        "!GRADIO_SHARE=1 llamafactory-cli webui\n",
        "# !CUDA_VISIBLE_DEVICES=0 python src/webui.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iESGXzgMJRO1"
      },
      "source": [
        "## Next:\n",
        "- choose model as: Mistral-7B-Instruct-v0.1\n",
        "- choose the dataset as: docker_dataset\n",
        "- set Quantization bit to: 4 to enable QLoRA.\n",
        "- set prompt template to mistral\n",
        "- set learning rate to 2e-4\n",
        "- set cut of length to 512 (to reduce computating cost and time).\n",
        "- reduce max samples to 10000 (to reduce computating cost and time).\n",
        "- set epochs to 1 (to reduce computating cost and time).\n",
        "- keep using bf16 (since we are not using a powerful gpu like A100).\n",
        "- change max gradient norm = 0.3.\n",
        "- set batch size to 16\n",
        "- LoRA configuration: usually it is set automatically by the source code so we will not change it. But to play with it you can increase LoRA Rank (intuition: the smaller the model is the higher the rank should be)\n",
        "- click on preview command to see all all parameters\n",
        "- click start and monitor the losses (losses will appear after a few minutes after the model is downloaded)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdgTXWZxfqmJ"
      },
      "source": [
        "## Merge with the base model and push to you HuggingFace hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aIBLgHBEVdS"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avms-7lxESOx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.1\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"/content/LLaMA-Factory/saves/Mistral-7B-v0.1/lora/train_2024-12-26-10-43-37\",            # load the saved LoRA adapters\n",
        "  template=\"mistral\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=1,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cuda\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  export_hub_model_id=\"Hghanem96/Mistral_docker\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_mistral_docker.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_mistral_docker.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZBS64YNDDsU"
      },
      "source": [
        "## Extra excercise: Try adding a new prompt template if the one of the model you finetune is not supported\n",
        "- Go to src/llamafactory/data/template\n",
        "- Templates are below in the code (ordered alphapetically by their name)\n",
        "- Add your template or modify an existing one, for example modify the following:\n",
        "```\n",
        "_register_template(\n",
        "    name=\"llama2_zh\",\n",
        "    format_user=StringFormatter(slots=[{\"bos_token\"}, \"[INST] {{content}} [/INST]\"]),\n",
        "    format_system=StringFormatter(slots=[\"<<SYS>>\\n{{content}}\\n<</SYS>>\\n\\n\"]),\n",
        "    default_system=\"You are a helpful assistant. 你是一个乐于助人的助手。\",\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llama2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
